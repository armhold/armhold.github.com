<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Google App Engine | George Armhold's Blog]]></title>
  <link href="http://armhold.github.com/blog/categories/google-app-engine/atom.xml" rel="self"/>
  <link href="http://armhold.github.com/"/>
  <updated>2012-08-21T11:43:38-04:00</updated>
  <id>http://armhold.github.com/</id>
  <author>
    <name><![CDATA[George Armhold]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to make Ajax links crawlable with GWT and Google App Engine]]></title>
    <link href="http://armhold.github.com/2011/01/07/how-to-make-ajax-links-crawlable-with-gwt-and-google-app-engine/"/>
    <updated>2011-01-07T13:18:11-05:00</updated>
    <id>http://armhold.github.com/2011/01/07/how-to-make-ajax-links-crawlable-with-gwt-and-google-app-engine</id>
    <content type="html"><![CDATA[<p>If you care about <a href="http://en.wikipedia.org/wiki/Search_engine_optimization">SEO</a> you know that using GWT has a downside: much of your app's content is generated dynamically via Javascript, and is therefore invisible to search engines. You might have dozens of pages of awesome content, but all the Googlebot sees is the static HTML page that hosts your app. This can be a real problem.</p>

<p>Fortunately, Google has proposed a <a href="http://code.google.com/web/ajaxcrawling/docs/getting-started.html">solution</a> for crawling ajax content:</p>

<ul>
<li><p>change your hrefs to support "bang notation":  www.example.com/ajax.html<strong>#!key=value</strong></p></li>
<li><p><strong>when Googlebot makes requests of the form: </strong>www.example.com/ajax.html<strong>?<em>escaped_fragment</em>=key=value </strong><strong>, you return a static HTML version of the Ajax content</strong></p></li>
</ul>


<p>So then the problem becomes one of generating static content from your Ajax links. You could do that by hand if your site is small and changes infrequently. More likely you'll want a way to automate this.</p>

<p>Google recommends using a "headless browser" approach, i.e. using something like <a href="http://htmlunit.sourceforge.net/">HtmlUnit</a>. That's a fine solution, but if you're running on App Engine <em>it's almost guaranteed not to work</em> because of the request timeout.  So if you want to run on App Engine, you're probably going to have to spider your own pages and pre-generate your HTML content.</p>

<p>My solution to this problem is to break the spidering up into small chunks, and farm them out as tasks on App Engine's <a href="http://code.google.com/appengine/docs/java/taskqueue/overview.html">Task Queues</a>. Whenever I update my app's content, I submit a job that spiders the landing page looking for Ajax links. For each link that's found, I submit a task that recursively spiders the link (taking care not to get into loops). Each task saves the HTML content into the data store, which is then returned as cached static content to Googlebot.</p>

<p>Suddenly my "simple" solution is sounding quite complicated, but it gets the job done reliably. Here's some code to make it clearer.</p>

<p>I use a CachedAjaxLink data object to persist the static content:</p>

<pre><code>public class CachedAjaxLink implements Serializable
{
    @Id
    private String href;
    private String cachedContent;
    private Date dateCached;
}
</code></pre>

<p>Then I use an AjaxCacher which crawls a given link, stores the results as CachedAjaxLinks, and queues Task Queue tasks for each link that it finds:</p>

<pre><code>public class AjaxCacher
{
    protected static final Logger log = Logger.getLogger(AjaxCacher.class.getName());
    protected static final DAO dao = new DAO();

    public static final long PUMP_TIME = 5000;
    protected WebClient webClient;
    protected String crawlServletUrl;

    public AjaxCacher(String crawlServletUrl)
    {
        this.crawlServletUrl = crawlServletUrl;
        webClient = Holder.get();
    }

    public void crawl(URL urlToCrawl, Date crawlRequestTimestamp)
    {
        // URLs we've already queued
        Set queuedURLs = new HashSet();
        queuedURLs.add(urlToCrawl);

        try
        {
            HtmlPage page = webClient.getPage(urlToCrawl);

            // appengine hack because it's single threaded
            webClient.getJavaScriptEngine().pumpEventLoop(PUMP_TIME);

            String pageContent = page.asXml();

            CachedAjaxLink cachedAjaxLink = new CachedAjaxLink();
            cachedAjaxLink.setHref(urlToCrawl.getRef());
            cachedAjaxLink.setCachedContent(pageContent);
            cachedAjaxLink.setDateCached(new Date());  // time actually cached
            dao.updateCachedAjaxLink(cachedAjaxLink);

            List anchors = page.getAnchors();
            for (HtmlAnchor anchor : anchors)
            {
                // only care about ajax links
                if (! anchor.getHrefAttribute().startsWith("#")) continue;

                URL newUrl = new URL(urlToCrawl, anchor.getHrefAttribute());

                // don't queue multiple requests for the same URL
                if (queuedURLs.contains(newUrl)) continue;

                queuedURLs.add(newUrl);

                // prevent loops
                CachedAjaxLink link = dao.getCachedAjaxLink(newUrl.getRef());
                if (link == null || link.getDateCached().getTime() &lt; crawlRequestTimestamp.getTime())
                {
                    queueCrawlRequest(newUrl.toString(), crawlRequestTimestamp);
                }
            }

        } catch (IOException e)
        {
            log.log(Level.SEVERE, e.getMessage(), e);
        }
        finally
        {
            webClient.closeAllWindows();
        }
    }

    /**
     * submits a crawl request to the queue; TaskQueueServlet will then handle the request asynchronously
     */
    public void queueCrawlRequest(String urlToCrawl, Date timeStamp)
    {
        Queue queue = QueueFactory.getDefaultQueue();
        TaskOptions options = TaskOptions.Builder.url(crawlServletUrl);
        options.param("encodedUrl", ServerUtils.encodeURL(urlToCrawl));
        options.param("timeStamp", ServerUtils.fromDate(timeStamp));
        options.method(TaskOptions.Method.GET);
        queue.add(options);
    }

    /**
     * try to cache a copy of the WebClient in ThreadLocal for faster startups on Google App Engine
     */
    public static class Holder
    {
        private static ThreadLocal holder = new ThreadLocal()
        {
            protected synchronized WebClient initialValue()
            {
                WebClient result = new WebClient(BrowserVersion.FIREFOX_3);
                result.setWebConnection(new UrlFetchWebConnection(result));
                return result;
            }
        };

        public static WebClient get()
        {
            return holder.get();
        }
    }
}
</code></pre>

<p>Finally, I use a TaskQueueServlet to handle the queued tasks:</p>

<pre><code>public class TaskQueueServlet extends HttpServlet
{
    @Override
    protected void doGet(HttpServletRequest req, HttpServletResponse res) throws ServletException, IOException
    {
        doPost(req, res);
    }

    @Override
    protected void doPost(HttpServletRequest req, HttpServletResponse res) throws ServletException, IOException
    {
        String encodedUrl = req.getParameter("encodedUrl");
        if (encodedUrl == null)
        {
            throw new IllegalArgumentException("missing param: encodedUrl");
        }

        String timeStamp = req.getParameter("timeStamp");
        if (timeStamp == null)
        {
            throw new IllegalArgumentException("missing param: timeStamp");
        }

        String decodedUrl = ServerUtils.decodeURL(encodedUrl);
        URL urlToCrawl = new URL(decodedUrl);

        getServletContext().getInitParameter("taskQueuePath");
        AjaxCacher cacher = new AjaxCacher(getServletContext().getInitParameter("taskQueuePath"));
        cacher.crawl(urlToCrawl, ServerUtils.toDate(timeStamp));
    }
}
</code></pre>

<p>Thanks Google, for making it so easy. ;-)</p>

<p>You can grab all of this code from my <a href="http://code.quickbrownfrog.com/">gwtquickstarter</a> library. It's the library that powers the <a href="http://www.quickbrownfrog.com">best typing tutor</a> on the web.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[how to resolve App Engine timeouts when parsing web.xml]]></title>
    <link href="http://armhold.github.com/2010/11/17/how-to-resolve-app-engine-timeouts-when-parsing-web-xml/"/>
    <updated>2010-11-17T12:09:48-05:00</updated>
    <id>http://armhold.github.com/2010/11/17/how-to-resolve-app-engine-timeouts-when-parsing-web-xml</id>
    <content type="html"><![CDATA[<p>For a few weeks now I've been plagued with timeouts while updating my App Engine apps. At first I thought it was a problem in the Intellij EAP beta I was running, but today I spent some time digging into it.</p>

<p>It seems to be unrelated to the EAP, as I get the same behavior with the App Engine command-line tools- timeout failure parsing web.xml roughly 75% of the time. It looks like this:</p>

<blockquote><p>Nov 17, 2010 4:32:54 PM com.google.apphosting.utils.config.AbstractConfigXmlReader readConfigXml
SEVERE: Received exception processing /Users/armhold/work/git-mega-repo/mystore/out/artifacts/Typing_Web_exploded/WEB-INF/web.xml
com.google.apphosting.utils.config.AppEngineConfigException: Received IOException parsing the input stream for /Users/armhold/work/git-mega-repo/mystore/out/artifacts/Typing_Web_exploded/WEB-INF/web.xml
at com.google.apphosting.utils.config.AbstractConfigXmlReader.getTopLevelNode(AbstractConfigXmlReader.java:210)
at com.google.apphosting.utils.config.AbstractConfigXmlReader.parse(AbstractConfigXmlReader.java:228)
at com.google.apphosting.utils.config.WebXmlReader.processXml(WebXmlReader.java:142)
at com.google.apphosting.utils.config.WebXmlReader.processXml(WebXmlReader.java:22)
at com.google.apphosting.utils.config.AbstractConfigXmlReader.readConfigXml(AbstractConfigXmlReader.java:111)
at com.google.apphosting.utils.config.WebXmlReader.readWebXml(WebXmlReader.java:73)
at com.google.appengine.tools.admin.Application.(Application.java:105)
at com.google.appengine.tools.admin.Application.readApplication(Application.java:151)
at com.google.appengine.tools.admin.AppCfg.(AppCfg.java:115)
at com.google.appengine.tools.admin.AppCfg.(AppCfg.java:61)
at com.google.appengine.tools.admin.AppCfg.main(AppCfg.java:57)
Caused by: java.net.ConnectException: Operation timed out
After some <a href="http://code.google.com/p/googleappengine/issues/detail?id=1235#c15">digging around</a>, it seems to be related to a timeout deep in the bowels of Java's XML libs while trying to validate the DTD for web.xml.</p></blockquote>

<p>The fix is simple: elide the DTD declaration from the top of your web.xml:</p>

<blockquote><p>&lt;!DOCTYPE web-app    PUBLIC "-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"    "http://java.sun.com/dtd/web-app_2_3.dtd"></p></blockquote>
]]></content>
  </entry>
  
</feed>
